{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam SMS Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hend-isleem/ml-spam-classification/blob/main/Spam_SMS_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7fKkdookDym"
      },
      "source": [
        "## 1 - Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivy3PlEM5OCO",
        "outputId": "3b2d7684-55e3-4698-d62b-e2b606f7dd4f"
      },
      "source": [
        "pip install -U nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAhb8Fxe3hv5"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import csv\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_rCotQWkDyw"
      },
      "source": [
        "## 2 - Overview of the Idea\n",
        "\n",
        " A Machine Learning classic beginner project using Python libraries to cluster a data set of 'sms' messages into 'spam' and 'ham' using **k-means**.\n",
        " \n",
        " The dataset is a collection of 5,574 SMS meesages taken from UCI Machine Learning repository, need to be tagged as \"spam\" and \"ham\".\n",
        "\n",
        " We use **tfidf approach** for text clustering, as it's proved to give a high prediction accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdwgx-Zv62a4"
      },
      "source": [
        "## 3 - Overview of the Work Pipeline:\n",
        "\n",
        "* Loading Data\n",
        "* Preprocessing\n",
        "* Feature Selection\n",
        "* Feature Vector Modelling \n",
        "* k-means clustering and evaluation\n",
        "* Showing Results "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsjbHHE17U1c"
      },
      "source": [
        "## **Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVYLCsKo_UlN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027bb16a-59d0-4f32-9820-f524b329dd90"
      },
      "source": [
        "#Reading the spam collection dataset\n",
        "def reading_training_file():\n",
        "    print(\"Reading messages from dataset...\")\n",
        "    documents = []\n",
        "    with open(\"sms_dataset.csv\") as csvfile:\n",
        "        rows = csv.reader(csvfile)\n",
        "        next(rows, None) #Skip column headers\n",
        "        for row in rows:\n",
        "            message = row[1]\n",
        "            documents.append(message) #messages are appended to the list 'documents'\n",
        "    print(\"Finished reading messages and appended to the list..\")\n",
        "    return documents\n",
        "print(f'length of documents: ', len(reading_training_file()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading messages from dataset...\n",
            "Finished reading messages and appended to the list..\n",
            "length of documents:  5574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1K9T4F_CPDq"
      },
      "source": [
        "## **Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSz3iuAPEO82"
      },
      "source": [
        "#Preprocessing steps including tokenization\n",
        "def preprocessing(documents):\n",
        "    print \"Precprocessing the messages for clustering...\"\n",
        "    vocab_glob = {}\n",
        "    tokenized_document = []\n",
        "    final_documents=[]\n",
        "    for document in documents:\n",
        "        text=document.replace(\"</p>\",\"\") # removing </p>\n",
        "        text=text.replace(\"<p>\",\" \")  # removing <p>\n",
        "        text = text.replace(\"http\", \" \")\n",
        "        text = text.replace(\"www\", \" \")\n",
        "        text = re.sub(r'([a-z])\\1+', r'\\1', text)\n",
        "        text = re.sub('\\s+', ' ', text)\n",
        "        text = re.sub('\\.+', '.', text)\n",
        "        text = re.sub(r\"(?:\\@|'|https?\\://)\\s+\",\"\",text) #delete punctuation\n",
        "        text = re.sub(\"[^a-zA-Z]\", \" \",text)\n",
        "        text=re.sub(r'[^\\w\\s]','',text) # remove punctuation\n",
        "        text=re.sub(\"\\d+\",\"\",text) # remove number from text\n",
        "        tokens_text = nltk.word_tokenize(text) # tokenizing the documents\n",
        "        stopwords=nltk.corpus.stopwords.words('english') #stopword reduction\n",
        "        tokens_text=[w for w in tokens_text if w.lower() not in stopwords]\n",
        "        tokens_text=[w.lower() for w in tokens_text] #convert to lower case\n",
        "        tokens_text=[w for w in tokens_text if len(w)>2] #considering tokens with length>2(meaningful words)\n",
        "        p= PorterStemmer() # stemming tokenized documents using Porter Stemmer\n",
        "        tokens_text = [p.stem(w) for w in tokens_text]\n",
        "        token_ind= []\n",
        "        counter=len(vocab_glob)-1\n",
        "        for token in tokens_text:\n",
        "         if token not in vocab_glob:\n",
        "            counter+=1\n",
        "            vocab_glob[token]=counter\n",
        "            token_ind.append(counter)\n",
        "         else:\n",
        "            token_ind.append(vocab_glob[token])\n",
        "        final_documents.append(token_ind)\n",
        "    print \"Finished pre-processing words..\"\n",
        "    return vocab_glob,final_documents\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}